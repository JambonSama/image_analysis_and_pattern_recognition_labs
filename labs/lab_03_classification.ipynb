{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2020:][iapr2020] Lab 3 â€’  Classification\n",
    "\n",
    "**Authors:** Claire Meyer, Nicolas Furrer, Philipp Schuler  \n",
    "**Due date:** 08.05.2020\n",
    "\n",
    "[iapr2020]: https://github.com/LTS5/iapr-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "# tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "# with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "#    tar.extractall(path=data_base_path)\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import scipy.io\n",
    "\n",
    "# Data loading\n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "# Size of data\n",
    "print(f\"Class A : {a.shape} \\nClass B : {b.shape} \\nClass c : {c.shape}\")\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method\n",
    "Using the Bayes method, give the analytical expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value.\n",
    "\n",
    "To implement the Bayers method in a 2D case, we use the discriminant function :\n",
    "\n",
    "$g_i(x) = - \\dfrac{1}{2}(x-\\mu_i)\\Sigma^{-1}_i(x-\\mu_i)^T+\\textrm{ln}P(w_i)+c_i$\n",
    "\n",
    "Where :\n",
    "- $x$ is the point to be classified\n",
    "- $\\mu_i$ is the mean of the $i$-th distribution\n",
    "- $\\Sigma_i$ is the covariance matrix of the $i$-th distribution\n",
    "- $P(w_i)$ is the propability of a point to by in class i (In our case this probability is equal to 1/3)\n",
    "- $c_i$ is a parameter who is equal to $\\textrm{ln}\\dfrac{1}{2\\pi|\\Sigma_i|^{1/2}}$\n",
    "\n",
    "Then to find the decision curve between to class we solve this equation :\n",
    "\n",
    "$g_i(x)-g_j(x) = 0$\n",
    "\n",
    "Where : \n",
    "- $g_i$is the discriminant function for the class i\n",
    "- $g_j$is the discriminant function for the class j\n",
    "\n",
    "This gives us the hyperplan equation that divides classes i and j. We can plot those hyperplans as decision curves. They represent decision curves for the complete Bayesian classification.\n",
    "\n",
    "From the complete Bayesian decision curves, we can compute the simplified decisions curves, for the simplified Bayesian classification. The process is explained later.\n",
    "\n",
    "\n",
    "When looking at the data of classes A, B and C, we can see that a diagonal covariance matrix is probably enough to model the data, but we decided to use a full covariance matrix for more precision, and because it has no drawback in our case (we have no computational limitations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sympy.solvers import solve\n",
    "from sympy import symbols\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random as rd\n",
    "\n",
    "# Computes the covariance matrix sigma.\n",
    "# Parameters : Data (200x2)(number mesure x dimension), mean of the data (1x2)(1 x dimension)\n",
    "# Return value : Covariance matrix sigma (2x2)(dimension x dimension)\n",
    "def covariance(data, data_mu):\n",
    "    X = data-data_mu\n",
    "    M = data.shape[0]\n",
    "    data_sigma = 1/M*(X.T @ X)\n",
    "    return data_sigma\n",
    "\n",
    "# Computes the value of the discriminant funtion for a data point.\n",
    "# Parameters : Point to evalute (1x2), Data (200x2)(number mesure x dimension), Propability to be in a class\n",
    "# Return value : Value of the discriminant function a the data_point\n",
    "def discriminantFonction(data_point, data, Pw):\n",
    "    data_mu = np.mean(data, axis = 0)\n",
    "    data_sigma = covariance(data, data_mu)\n",
    "    c = np.log(2*np.pi*np.sqrt(np.linalg.det(data_sigma)))\n",
    "    g =  -1/2*((data_point-data_mu)@np.linalg.inv(data_sigma)@(data_point-data_mu).T)+np.log(Pw)-c\n",
    "    return g\n",
    "\n",
    "# Compute the class of a data_point in function of 3 class of point .\n",
    "# Parameters : Point to evalute (1x2), 3 class data  (200x2)(number mesure x dimension), Propability to be in a class\n",
    "# Return value : the Class where the point is, A,B or C\n",
    "def completeBayesClassification(data_point, class_a, class_b, class_c, Pw):\n",
    "    dist = np.zeros(3)\n",
    "    dist[0] = discriminantFonction(data_point, class_a, Pw)\n",
    "    dist[1] = discriminantFonction(data_point, class_b, Pw)\n",
    "    dist[2] = discriminantFonction(data_point, class_c, Pw)\n",
    "    \n",
    "    return chr(np.argmax(dist)+65) # to get a, b or c class\n",
    "\n",
    "# Compute the class of a data_point in function of 3 class of point with the simplified decision curve.\n",
    "# Parameters : Point to evalute (1x2), 3 equation value for each class\n",
    "# Return value : the Class where the point is, A,B or C\n",
    "def simplifiedBayesClassification(data_point, equ1, equ2, equ3):\n",
    "    if data_point[1] > equ1:\n",
    "        if data_point[0] > equ2:\n",
    "            return 'B'\n",
    "        else:\n",
    "            return 'C'\n",
    "    else:\n",
    "        if data_point[0] > equ3:\n",
    "            return 'A'\n",
    "        else:\n",
    "            return 'C'\n",
    "\n",
    "# Computes different parameter : mean, sigma inverse and parameter c for equation page 12 Cours 5.\n",
    "# Parameter : Data (200x2)(number mesure x dimension)\n",
    "# Return value : mean (1x2)(1 x dimension, inverse matrix sigma (2x2)(dimension x dimension), parameter c (1x1)\n",
    "def computation(data):\n",
    "    mean = np.mean(data, axis = 0)\n",
    "    sigma = covariance(data, mean)\n",
    "    sigma_det = np.linalg.det(sigma)\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    c = np.log(2*np.pi*np.sqrt(sigma_det))\n",
    "    return mean, sigma_inv, c\n",
    "\n",
    "# Finds the decision curves between two data set.\n",
    "# Parameters : 2 Data (200x2)(number mesure x dimension), choice_variable 0 : x =, 1 : y =\n",
    "# Return value : equation of decision curves (2 equation)\n",
    "def decisionCurve(data_i, data_j, choice_variable):\n",
    "    \n",
    "    mean_i, sigma_i_inv, c_i = computation(data_i) \n",
    "    mean_j, sigma_j_inv, c_j = computation(data_j) \n",
    "    \n",
    "    x, y = symbols('x, y', integer=True)\n",
    "    X = np.array([[x,y]])\n",
    "    \n",
    "    g_i = -1/2*((X-mean_i)@sigma_i_inv@(X-mean_i).T)+np.log(1/3)-c_i\n",
    "    g_j = -1/2*((X-mean_j)@sigma_j_inv@(X-mean_j).T)+np.log(1/3)-c_j\n",
    "    g = g_i-g_j\n",
    "    \n",
    "    if choice_variable:\n",
    "        equ = solve(g[0][0],x)\n",
    "    else:\n",
    "        equ = solve(g[0][0],y)\n",
    "    return equ\n",
    "\n",
    "# Evaluates a equation with the a vector of value x0.\n",
    "# Parameters : 1 equation with x as a parameter, vector of value to evaluate the equation\n",
    "# Return value : vector of value of y\n",
    "def evaluateXY(equation, input_value):\n",
    "    y = symbols('y', integer=True)\n",
    "    output_value = np.empty(input_value.shape[0])\n",
    "    for i in range(input_value.shape[0]):\n",
    "        output_value[i] = equation.subs(y, input_value[i])\n",
    "    \n",
    "    return output_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision curves equations\n",
    "eq_comp_1 = decisionCurve(a,b,1)\n",
    "eq_comp_2 = decisionCurve(b,c,1)\n",
    "eq_comp_3 = decisionCurve(c,a,1)\n",
    "\n",
    "# Simplified decision_curve\n",
    "x, y = symbols('x, y', integer=True)\n",
    "eq_simp_1_x = round(decisionCurve(a,b,0)[1].subs(x,0))\n",
    "eq_simp_2_y = round(eq_comp_2[1].subs(y,0))\n",
    "eq_simp_3_y = round(eq_comp_3[1].subs(y,0))\n",
    "\n",
    "# Computation of XY for graph\n",
    "step_value = 0.1\n",
    "\n",
    "# Full hyperplans\n",
    "y_full_1 = np.arange(start=-25, stop=5,step=step_value)\n",
    "y_full_2 = np.arange(start=-40, stop=25,step=step_value)\n",
    "y_full_3 = np.arange(start=-40, stop=25,step=step_value)\n",
    "\n",
    "x_full_1_1 = evaluateXY(eq_comp_1[0],y_full_1)\n",
    "x_full_1_2 = evaluateXY(eq_comp_1[1],y_full_1)\n",
    "\n",
    "x_full_2_1 = evaluateXY(eq_comp_2[0],y_full_2)\n",
    "x_full_2_2 = evaluateXY(eq_comp_2[1],y_full_2)\n",
    "\n",
    "x_full_3_1 = evaluateXY(eq_comp_3[0],y_full_3)\n",
    "x_full_3_2 = evaluateXY(eq_comp_3[1],y_full_3)\n",
    "\n",
    "# Intersected hyperplans\n",
    "y_inter_1_1 = np.concatenate(([2.71],np.arange(start=2.8,stop=5,step=step_value)))\n",
    "y_inter_1_2 = np.concatenate(([-21.217],np.arange(start=-21.1,stop=2.1,step=step_value),[2.109]))\n",
    "\n",
    "y_inter_2_1 = np.arange(start=2.8,stop=25,step=step_value)\n",
    "y_inter_2_2 = np.arange(start=2.1,stop=25,step=step_value)\n",
    "\n",
    "y_inter_3_1 = np.arange(start=-40,stop=2.8,step=step_value)\n",
    "y_inter_3_2 = np.arange(start=-40,stop=2.15,step=step_value)\n",
    "\n",
    "x_inter_1_1 = evaluateXY(eq_comp_1[1],y_inter_1_1)\n",
    "x_inter_1_2 = evaluateXY(eq_comp_1[1],y_inter_1_2)\n",
    "\n",
    "x_inter_2_1 = evaluateXY(eq_comp_2[0],y_inter_2_1)\n",
    "x_inter_2_2 = evaluateXY(eq_comp_2[1],y_inter_2_2)\n",
    "\n",
    "x_inter_3_1 = evaluateXY(eq_comp_3[0],y_inter_3_1)\n",
    "x_inter_3_2 = evaluateXY(eq_comp_3[1],y_inter_3_2)\n",
    " \n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation of decision_curve\n",
    "print(f\"Round equation between class A and B :\")\n",
    "print(f\"\\ty = {eq_simp_1_x}\")\n",
    "print(f\"Round equation between class B and C :\")\n",
    "print(f\"\\tx = {eq_simp_2_y}\")\n",
    "print(f\"Round equation between class C and A :\")\n",
    "print(f\"\\tx = {eq_simp_3_y}\")\n",
    "\n",
    "# Plot images\n",
    "n_lin = 3\n",
    "n_col = 1\n",
    "size = 15\n",
    "alpha_val = 0.2\n",
    "fig, ax = plt.subplots(n_lin, n_col, figsize=(n_col*size, n_lin*size/2))\n",
    "\n",
    "ax[0].plot(a[:,0],a[:,1],\".\",color=\"#0072BD\")\n",
    "ax[0].plot(b[:,0],b[:,1],\".\",color=\"#A2142F\")\n",
    "ax[0].plot(c[:,0],c[:,1],\".\",color=\"#EDB120\")\n",
    "\n",
    "ax[0].plot(x_full_1_1,y_full_1,color=\"#7E2F8E\")\n",
    "ax[0].plot(x_full_2_1,y_full_2,color=\"#D95319\")\n",
    "ax[0].plot(x_full_3_1,y_full_3,color=\"#77AC30\")\n",
    "\n",
    "ax[0].plot(x_full_1_2,y_full_1,color=\"#7E2F8E\")\n",
    "ax[0].plot(x_full_2_2,y_full_2,color=\"#D95319\")\n",
    "ax[0].plot(x_full_3_2,y_full_3,color=\"#77AC30\")\n",
    "\n",
    "ax[0].set_title(\"Classes A, B and C 2D representation with complete Bayesian hyperplans\")\n",
    "ax[0].legend((\"Class A\",\"Class B\",\"Class C\",\"AB decision curves\",\"BC decision curves\",\"AC decision curves\"))\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(a[:,0],a[:,1],\".\",color=\"#0072BD\")\n",
    "ax[1].plot(b[:,0],b[:,1],\".\",color=\"#A2142F\")\n",
    "ax[1].plot(c[:,0],c[:,1],\".\",color=\"#EDB120\")\n",
    "\n",
    "ax[1].plot(x_inter_1_1,y_inter_1_1,color=\"#7E2F8E\")\n",
    "ax[1].plot(x_inter_2_1,y_inter_2_1,color=\"#D95319\")\n",
    "ax[1].plot(x_inter_3_1,y_inter_3_1,color=\"#77AC30\")\n",
    "\n",
    "ax[1].plot(x_inter_1_2,y_inter_1_2,color=\"#7E2F8E\")\n",
    "ax[1].plot(x_inter_2_2,y_inter_2_2,color=\"#D95319\")\n",
    "ax[1].plot(x_inter_3_2,y_inter_3_2,color=\"#77AC30\")\n",
    "\n",
    "ax[1].set_xlim([-20,20])\n",
    "ax[1].set_ylim([-10,10])\n",
    "\n",
    "ax[1].set_title(\"Classes A, B and C 2D representation with complete Bayesian decision curves\")\n",
    "ax[1].legend((\"Class A\",\"Class B\",\"Class C\",\"AB decision curves\",\"BC decision curves\",\"AC decision curves\"))\n",
    "\n",
    "\n",
    "\n",
    "ax[2].plot(a[:,0],a[:,1],\".\",color=\"#0072BD\")\n",
    "ax[2].plot(b[:,0],b[:,1],\".\",color=\"#A2142F\")\n",
    "ax[2].plot(c[:,0],c[:,1],\".\",color=\"#EDB120\")\n",
    "\n",
    "ax[2].plot([eq_simp_3_y,20],[eq_simp_1_x,eq_simp_1_x],color=\"#7E2F8E\")\n",
    "ax[2].plot([eq_simp_2_y,eq_simp_2_y],[eq_simp_1_x,30],color=\"#D95319\")\n",
    "ax[2].plot([eq_simp_3_y,eq_simp_3_y],[-25,eq_simp_1_x],color=\"#77AC30\")\n",
    "\n",
    "ax[2].set_xlim([-20,20])\n",
    "ax[2].set_ylim([-10,10])\n",
    "\n",
    "ax[2].set_title(\"Classes A, B and C 2D representation with simplified Bayesian decision curves\")\n",
    "ax[1].legend((\"Class A\",\"Class B\",\"Class C\",\"AB decision curves\",\"BC decision curves\",\"AC decision curves\"))\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above Figure are plotted the decisions hyperplans : \n",
    "- On the first, we have the hyperplans corresponding to the complete Bayesian classification. They are quadratic, so there are two curves for each hyperplan (for a total of 2x3=6 curves). \n",
    "- On the second, we have the boundaries between the three classes, defined by the complete Bayesian hyperplan. They are intersecting as illustrated because the discrimination criteria is based on the maximum value that the disciminant function takes for the three classes at the considered point.\n",
    "- On the third, we have the simplified curves. They are obtained by taking the equations (for example equation between classes A and B) :\n",
    "\n",
    "$y=-0.00270195666350582\\cdot x \\pm 11.786193018351\\cdot \\sqrt{-3.51523290159705e-5\\cdot x^2 - 0.0128778860105878 \\cdot x + 1}- 10.1975370992027$\n",
    "\n",
    "We round each parameter to its closest integer :\n",
    "\n",
    "$y=-0\\cdot x \\pm 12\\cdot \\sqrt{-0\\cdot x^2 - 0 \\cdot x + 1}- 10$\n",
    "\n",
    "$y=\\pm 12 - 10$\n",
    "\n",
    "Then, we keep the most \"logical\" one (by looking a the complete Bayesian decision curves and the data), which give us three equations :\n",
    "- Between A and B : $y=+2$\n",
    "- Between B and C : $x=-6$\n",
    "- Between C and A : $x=-8$\n",
    "\n",
    "The first two plots of the above Figure, we can see how the hyperplans separate the data, and the consequences of them being quadratic. Let's take the AB decision curves (and **ignore the class C and its own boudaries**). The curves we are looking at are the two violet hyperbole looking ones. Inside the hyperbole is the class A, and outside is the class B. That means that as long as we stay close to the original datasets, the classification is fairly accurate and similar as the one a human would give. But if we get far away from the original dataset, the results start to differ to the ones a human would give, because we don't \"draw\" the other part of the quadratic solution \"in our heads\". It doesn't mean that the classified solution are wrong (or right), though, just that trying to classify points that are so remote from the original dataset would be inadequate. That's because if testing points were to be generated the same way training points had been generated, they wouldn't be so far away from the original dataset.\n",
    "\n",
    "On the last plot of the above Figure, that behavior is not present, because we only kept the \"logical\" part. Again, \"logical\" is used loosely here, because it's only logical to us humans since we extrapolate the data a certain way, but points far removed from the original data set do not have a right or wrong label, they are just inadequate for the classification based on training with the given dataset.\n",
    "\n",
    "More quantified comparisons as well as analysis are given later, as to include the Mahalanobis distance classifier as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance is defined by :\n",
    "\n",
    "$d_m = \\left( \\left( x-\\mu_i \\right) \\Sigma^{-1}_{i} \\left( x-\\mu_i \\right)^T \\right)^{1/2}$\n",
    "\n",
    "Where :\n",
    "- $x$ is the point to be classified\n",
    "- $\\mu_i$ is the mean of the $i$-th distribution\n",
    "- $\\Sigma_i$ is the covariance matrix of the $i$-th distribution\n",
    "\n",
    "The Mahalanobis distance is a measure of the distance between a point P and a distribution D, introduced by P. C. Mahalanobis in 1936. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along each principal component axis. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set [[1]][maha].\n",
    "\n",
    "[maha]: https://en.wikipedia.org/wiki/Mahalanobis_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the Mahalanobis distance between a data point and a data set. \n",
    "# A GMM is used on the data set (mu and sigma) to compute the distance.\n",
    "# Parameters : point the data point, data, the list of points in the data set\n",
    "# Return value : the Mahalanobis distance\n",
    "def mahalanobisDistance(point, data):\n",
    "    mean_value = np.mean(data, axis = 0)\n",
    "    sigma_value = covariance(data, mean_value)\n",
    "    return np.sqrt((point-mean_value)@np.linalg.inv(sigma_value)@(point-mean_value).T)\n",
    "\n",
    "# Computes to which data set the data point belongs to, using the Mahalanobis distance.\n",
    "# Parameters : the data point and the three candidate data sets\n",
    "# Return value : the name of the class the data point belongs to\n",
    "def mahalanobisClassification(point, class_a, class_b, class_c):\n",
    "    dist = np.zeros(3)\n",
    "    dist[0] = mahalanobisDistance(point, class_a)\n",
    "    dist[1] = mahalanobisDistance(point, class_b)\n",
    "    dist[2] = mahalanobisDistance(point, class_c)\n",
    "    \n",
    "    return chr(np.argmin(dist)+65) # to get a, b or c class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images\n",
    "n_lin = 2\n",
    "n_col = 2\n",
    "size = 10\n",
    "fig, ax = plt.subplots(n_lin, n_col, figsize=(n_col*size, n_lin*size))\n",
    "for i in range(n_lin):\n",
    "    for j in range(n_col):\n",
    "        ax[i][j].plot(a[:,0],a[:,1],\".\",color=\"#0072BD\")\n",
    "        ax[i][j].plot(b[:,0],b[:,1],\".\",color=\"#A2142F\")\n",
    "        ax[i][j].plot(c[:,0],c[:,1],\".\",color=\"#EDB120\")\n",
    "        ax[i][j].plot(x_inter_1_1,y_inter_1_1,color=\"#7E2F8E\")\n",
    "        ax[i][j].plot(x_inter_2_1,y_inter_2_1,color=\"#D95319\")\n",
    "        ax[i][j].plot(x_inter_3_1,y_inter_3_1,color=\"#77AC30\")\n",
    "\n",
    "# Test points\n",
    "Pw = 1/3\n",
    "range_num = 20\n",
    "\n",
    "print(f\"Point testing results :\")\n",
    "print(f\"\\tfor points in 'reasonable positions' around class A:\")\n",
    "print(f\"\\t\\tPoint\\t\\t\\tComplete Bayesian classification\\tSimplified Bayesian classification\\tMahalanobis distance classification\")\n",
    "for i in range(range_num):\n",
    "    x = rd.uniform(-5,15)\n",
    "    y = rd.uniform(-6,1)\n",
    "    point = np.array([[x,y]])\n",
    "    ax[0][0].plot(x,y,\".\",color=\"k\",markersize=10)\n",
    "    print(f\"\\t\\t[{point[[0][0]][0]:6.2f},{point[[0][0]][1]:6.2f}]\\t\\tclass {completeBayesClassification(point,a,b,c,Pw)}\\t\\t\\t\\t\\tclass {simplifiedBayesClassification([x,y],eq_simp_1_x,eq_simp_2_y,eq_simp_3_y)}\\t\\t\\t\\t\\tclass {mahalanobisClassification(point,a,b,c)}\")\n",
    "    \n",
    "\n",
    "print(f\"\\n\\tfor points in 'reasonable positions' around class B:\")\n",
    "print(f\"\\t\\tPoint\\t\\t\\tComplete Bayesian classification\\tSimplified Bayesian classification\\tMahalanobis distance classification\")\n",
    "for i in range(range_num):\n",
    "    x = rd.uniform(-5,15)\n",
    "    y = rd.uniform(1,10)\n",
    "    point = np.array([[x,y]])\n",
    "    ax[0][1].plot(x,y,\".\",color=\"k\",markersize=10)\n",
    "    print(f\"\\t\\t[{point[[0][0]][0]:6.2f},{point[[0][0]][1]:6.2f}]\\t\\tclass {completeBayesClassification(point,a,b,c,Pw)}\\t\\t\\t\\t\\tclass {simplifiedBayesClassification([x,y],eq_simp_1_x,eq_simp_2_y,eq_simp_3_y)}\\t\\t\\t\\t\\tclass {mahalanobisClassification(point,a,b,c)}\")\n",
    "\n",
    "print(f\"\\n\\tfor points in 'reasonable positions' around class C:\")\n",
    "print(f\"\\t\\tPoint\\t\\t\\tComplete Bayesian classification\\tSimplified Bayesian classification\\tMahalanobis distance classification\")\n",
    "for i in range(range_num):\n",
    "    x = rd.uniform(-15,-5)\n",
    "    y = rd.uniform(-15,15)\n",
    "    point = np.array([[x,y]])\n",
    "    ax[1][0].plot(x,y,\".\",color=\"k\",markersize=10)\n",
    "    print(f\"\\t\\t[{point[[0][0]][0]:6.2f},{point[[0][0]][1]:6.2f}]\\t\\tclass {completeBayesClassification(point,a,b,c,Pw)}\\t\\t\\t\\t\\tclass {simplifiedBayesClassification([x,y],eq_simp_1_x,eq_simp_2_y,eq_simp_3_y)}\\t\\t\\t\\t\\tclass {mahalanobisClassification(point,a,b,c)}\")\n",
    "\n",
    "print(f\"\\n\\tfor points in 'unreasonable positions' below classes A and C:\")\n",
    "print(f\"\\t\\tPoint\\t\\t\\tComplete Bayesian classification\\tSimplified Bayesian classification\\tMahalanobis distance classification\")\n",
    "for i in range(range_num):\n",
    "    x = rd.uniform(-15,15)\n",
    "    y = rd.uniform(-10,-40)\n",
    "    point = np.array([[x,y]])\n",
    "    ax[1][1].plot(x,y,\".\",color=\"k\",markersize=10)\n",
    "    print(f\"\\t\\t[{point[[0][0]][0]:6.2f},{point[[0][0]][1]:6.2f}]\\t\\tclass {completeBayesClassification(point,a,b,c,Pw)}\\t\\t\\t\\t\\tclass {simplifiedBayesClassification([x,y],eq_simp_1_x,eq_simp_2_y,eq_simp_3_y)}\\t\\t\\t\\t\\tclass {mahalanobisClassification(point,a,b,c)}\")\n",
    "\n",
    "# Plots legends and title\n",
    "for i in range(n_lin):\n",
    "    for j in range(n_col):\n",
    "        ax[i][j].plot(x_inter_1_2,y_inter_1_2,color=\"#7E2F8E\")\n",
    "        ax[i][j].plot(x_inter_2_2,y_inter_2_2,color=\"#D95319\")\n",
    "        ax[i][j].plot(x_inter_3_2,y_inter_3_2,color=\"#77AC30\")\n",
    "        ax[i][j].legend((\"Class A\",\"Class B\",\"Class C\",\"AB decision curves\",\"BC decision curves\",\"AC decision curves\",\"Tested points\"))\n",
    "\n",
    "ax[0][0].set_xlim([-20,20])\n",
    "ax[0][0].set_ylim([-15,15])\n",
    "ax[0][1].set_xlim([-20,20])\n",
    "ax[0][1].set_ylim([-15,15])\n",
    "ax[1][0].set_xlim([-20,20])\n",
    "ax[1][0].set_ylim([-15,15])\n",
    "ax[1][1].set_xlim([-20,20])\n",
    "ax[1][1].set_ylim([-40,10])\n",
    "\n",
    "ax[0][0].set_title(\"Classes A, B and C 2D representation with class A tested points\")\n",
    "ax[0][1].set_title(\"Classes A, B and C 2D representation with class B tested points\")\n",
    "ax[1][0].set_title(\"Classes A, B and C 2D representation with class C tested points\")\n",
    "ax[1][1].set_title(\"Classes A, B and C 2D representation with tested points below classes A and C\")\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above results, we observe that :\n",
    "- For points that are relevant to the initial data sets, the classification is mostly correct (see quantification below), for all three methods (complete Bayesian classification, simplified Bayesian classification, and Mahalanobis distance classification). That shows that the models chosen to modelize the data are relevant models.\n",
    "- For points far removed from the original data sets, the classification is :\n",
    "    - the same between complete Bayesian and Mahalanobis distance classification methods; \n",
    "    - but is different for the simplified Bayesian classification. \n",
    "- That's because if we neglect the constants, the complete Bayesian discrimant can be simplified as the Mahalanobis distance; and here, the constants are negligeable; so both yields the same results. But the simplified Bayesian classification has drastically different decision curves far removed from the original datasets, so the results are different.\n",
    "- For points far removed from the original data sets, it makes little sense to try and classify them, but again :\n",
    "    - complete Bayesian and Mahalanobis distance classification methods yields the same results;\n",
    "    - simplified Bayesian classification produces different results because of different decision curve.\n",
    "- It makes little sense, because those points do not belong to any of the classes; if the testing points were generated the same way as the training points, they would not be so far removed from the original, training data sets. So the classification produced is neither right nor wrong; it's just inadequate.\n",
    "\n",
    "\\\n",
    "\\\n",
    "To assess the validity of those results, let's put aside that the entirety of the datasets were used for training, and use them for testing, and compute the error that we yield this way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes error between data set and label.\n",
    "# Parameters : data sets (200x2), label ('A', 'B' or 'C')\n",
    "# Return value : error value for bayes_complet, mahalaobis and bayes_simplified\n",
    "def errorComputation(data, class_char):\n",
    "    error_bayes_complete = 0\n",
    "    error_mahalanobis = 0\n",
    "    error_bayes_simplified = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        if completeBayesClassification(data[i,:],a,b,c,Pw) != class_char:\n",
    "            error_bayes_complete = error_bayes_complete + 1\n",
    "        if simplifiedBayesClassification(data[i,:], eq_simp_1_x, eq_simp_2_y, eq_simp_3_y) != class_char:\n",
    "            error_bayes_simplified = error_bayes_simplified + 1\n",
    "        if mahalanobisClassification(data[i,:],a,b,c) != class_char:\n",
    "            error_mahalanobis = error_mahalanobis + 1\n",
    "    return [error_bayes_complete/data.shape[0], error_bayes_simplified/data.shape[0], error_mahalanobis/data.shape[0]]\n",
    "\n",
    "# Error print\n",
    "error = np.zeros((4,3))\n",
    "error[0,:] = errorComputation(a, 'A')\n",
    "error[1,:] = errorComputation(b, 'B')\n",
    "error[2,:] = errorComputation(c, 'C')\n",
    "error[3,:] = error.sum(0)/3\n",
    "\n",
    "print(f\"Classification error\")\n",
    "print(f\"\\t\\t\\t|Complete Bayes classification\\t|Simplified Bayes classification\\t|Mahalanobis distance classification\")\n",
    "print(\"\\t________________|_______________________________|_______________________________________|___________________________________\")\n",
    "print(f\"\\tClass A\\t\\t|{error[0][0]:5.3f}\\t\\t\\t\\t|{error[0][1]:5.3f}\\t\\t\\t\\t\\t|{error[0][2]:5.3f}\")\n",
    "print(f\"\\tClass B\\t\\t|{error[1][0]:5.3f}\\t\\t\\t\\t|{error[1][1]:5.3f}\\t\\t\\t\\t\\t|{error[1][2]:5.3f}\")\n",
    "print(f\"\\tClass C\\t\\t|{error[2][0]:5.3f}\\t\\t\\t\\t|{error[2][1]:5.3f}\\t\\t\\t\\t\\t|{error[2][2]:5.3f}\")\n",
    "print(f\"\\tAll classes\\t|{error[3][0]:5.3f}\\t\\t\\t\\t|{error[3][1]:5.3f}\\t\\t\\t\\t\\t|{error[3][2]:5.3f}\")\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification error on the datasets confirm the previous discussions. The simplified Bayesian classification has an overall higher error rate (6.2% of error rate) than the other two (3.5%). That's because the simplified Bayesian is, well, simplified, and since the training sets (which are also here the testing sets) are overlapping a little, the simplified Bayes classification fares a little less well than the other two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python.\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)\n",
    "    \n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "SciKit proposes a multi-layer perceptron classifier function 'MLPClassifier'. \n",
    "This classifier proposes different optimisers and activation functions. \n",
    "We tested all of them to then decide the best classifier for our use case:\n",
    "\n",
    " \n",
    " | Optimiser | Loss   | Training score | Test score |\n",
    " |-----------|-------:|:--------------:|:----------:|\n",
    " | Adam      | 0.05   |  0.9899        | 0.962      |\n",
    " | SGD       | 0.23   |  0.9246        | 0.917      |\n",
    " | L-BFGS    | 120.51 |  0.1169        | 0.108      |\n",
    " \n",
    "[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) is not optimised for large training sets and this is shown by the tests, as it output random values of loss and score at each iteration. \n",
    "\n",
    "[SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) or stochastic gradient descent, is working but takes much more steps to converge than Adam. \n",
    "\n",
    "As [Adam](https://www.deeplearning-academy.com/p/ai-wiki-optimization-algorithms) has the best results without optimisation, and SGD seems to not converging very quickly (200 epochs and not converged) we will use Adam for the nexts steps and try to optimise this one\n",
    "\n",
    "The default parameters are: activation function = \"Relu\", $\\alpha = 10^{-4}$, $\\beta_2 = 0.999$ and  $\\beta_1 = 0.9$. \n",
    "\n",
    "We iterate through each parameter, keeping the best result for the next step.\n",
    " \n",
    "First, we try out different activation functions:\n",
    "\n",
    "\n",
    "| Activation function | Loss | Training Score | Test Score |\n",
    "|---------------------|------|----------------|------------|\n",
    "| Identity            | 0.37 | 0.9155         | 0.906      |\n",
    "| Logistic            | 0.17 | 0.9522         | 0.947      |\n",
    "| Tanh                | 0.19 | 0.9469         | 0.942      |\n",
    "| Relu                | 0.05 | 0.9899         | 0.962      |\n",
    "\n",
    "With these results we choose to keep the Relu activation function as it shows the best restults out of the box.\n",
    "\n",
    "\n",
    "Then we test different $\\alpha$ values:\n",
    "\n",
    "\n",
    "| $\\alpha$ Values     | Loss | Training Score | Test Score |\n",
    "|---------------------|------|----------------|------------|\n",
    "| 0.001               | 0.04 | 0.9927         | 0.967      |\n",
    "| 0.0001              | 0.05 | 0.9899         | 0.962      |\n",
    "| 0.00001             | 0.05 | 0.9893         | 0.965      |\n",
    "\n",
    "As show, the best results are with $\\alpha = 0.001$\n",
    "\n",
    "Keeping the $\\alpha$ value, we try different $\\beta_2$ values:\n",
    "\n",
    "| $\\beta_2$ Values    | Loss | Training Score | Test Score |\n",
    "|---------------------|------|----------------|------------|\n",
    "| 0.99                | 0.03 | 0.9894         | 0.965      |\n",
    "| 0.999               | 0.04 | 0.9927         | 0.967      |\n",
    "| 0.9999              | 0.05 | 0.9933         | 0.966      |\n",
    "| 0.99999             | 0.00 | 0.9961         | 0.963      |\n",
    "| 0.999999            | 0.01 | 0.9911         | 0.956      |\n",
    "\n",
    "Now we keep $\\beta_2 = 0.99999$\n",
    "\n",
    "Finally we test different $\\beta_1$ values:\n",
    "\n",
    "| $\\beta_1$ Values    | Loss | Training Score | Test Score |\n",
    "|---------------------|------|----------------|------------|\n",
    "| 0.1                 | 0.03 | 0.9945         | 0.966      |\n",
    "| 0.9                 | 0.01 | 0.9961         | 0.963      |\n",
    "| 0.99                | 0.03 | 0.9825         | 0.948      |\n",
    "| 0.999               | 0.12 | 0.9603         | 0.949      |\n",
    "\n",
    "In the end we choose to stay with the Relu activation function, $\\alpha = 0.001$, $\\beta_1 = 0.9$ and $\\beta_2 = 0.99999$, which has \n",
    "\n",
    "#### Result output explanation\n",
    "The result output shows the results for each model (Adam, SGD and L-BFGS) with their respective losses, training score and test score. \n",
    "\n",
    "The test score is given by the percentage of characters correctly classified by the neural network.\n",
    "\n",
    "The graph shows the loss curve of SGD and Adam. This is the loss of each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def trainClassifiers():\n",
    "    #definition of the used models\n",
    "    model_lbfgs = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100), max_iter=200, random_state=1)\n",
    "\n",
    "    model_adam_1 = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=1e-3, batch_size='auto', \n",
    "                    learning_rate='constant', max_iter=200, shuffle=True, random_state=1, \n",
    "                    tol=0.0001, verbose=False, warm_start=False, early_stopping=True, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.99999, epsilon=1e-08, n_iter_no_change=10)\n",
    "    \n",
    "    model_sgd = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='sgd', alpha=1e-4, batch_size='auto', \n",
    "                    learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=1, \n",
    "                    tol=0.0001, verbose=False, warm_start=False, early_stopping=True, momentum=0.9, nesterovs_momentum=True,\n",
    "                    validation_fraction=0.1, n_iter_no_change=10)\n",
    "\n",
    "\n",
    "    model_lbfgs.fit(flattened_training_set, train_labels)\n",
    "    model_adam_1.fit(flattened_training_set, train_labels)\n",
    "    model_sgd.fit(flattened_training_set, train_labels)\n",
    "    \n",
    "    return model_lbfgs, model_sgd, model_adam_1\n",
    "    \n",
    "def computeTestScore(model):\n",
    "    output = model.predict(flattened_testing_set)\n",
    "    nb_wrong = 0\n",
    "    idx_list = []\n",
    "    for idx, predicted in enumerate(output):\n",
    "        if predicted != test_labels[idx]:\n",
    "            nb_wrong = nb_wrong +1 \n",
    "            idx_list.append(idx)\n",
    "\n",
    "    return 1-(nb_wrong/test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results :\n",
      "\tAdam :\tloss =   0.03, training score = 0.9945, test score = 0.966\n",
      "\tSGD :\tloss =   0.15, training score = 0.9570, test score = 0.941\n",
      "\tLBFGS :\tloss = 120.51, training score = 0.1169, test score = 0.108\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flattening training and testing set\n",
    "flattened_training_set = train_images.reshape(train_set_size, image_shape[0]*image_shape[1])\n",
    "flattened_testing_set = test_images.reshape(test_set_size,image_shape[0]*image_shape[1])\n",
    "\n",
    "# Train classifiers\n",
    "model_lbfgs, model_sgd, model_adam_1 = trainClassifiers()\n",
    "\n",
    "# Compute test scores\n",
    "test_score_lbfgs = computeTestScore(model_lbfgs)\n",
    "test_score_adam_1 = computeTestScore(model_adam_1)\n",
    "test_score_sgd = computeTestScore(model_sgd)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model results :\")\n",
    "print(f\"\\tAdam :\\tloss = {model_adam_1.loss_:6.2f}, training score = {model_adam_1.score(flattened_training_set, train_labels):6.4f}, test score = {test_score_adam_1:5.3f}\")\n",
    "print(f\"\\tSGD :\\tloss = {model_sgd.loss_:6.2f}, training score = {model_sgd.score(flattened_training_set, train_labels):6.4f}, test score = {test_score_sgd:5.3f}\")\n",
    "print(f\"\\tLBFGS :\\tloss = {model_lbfgs.loss_:6.2f}, training score = {model_lbfgs.score(flattened_training_set, train_labels):6.4f}, test score = {test_score_lbfgs:5.3f}\")\n",
    "\n",
    "# Plots\n",
    "plt.plot(model_adam_1.loss_curve_)\n",
    "plt.plot(model_sgd.loss_curve_)\n",
    "plt.legend([\"Adam\", \"SGD\"])\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
