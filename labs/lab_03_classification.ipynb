{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR 2020:][iapr2020] Lab 3 â€’  Classification\n",
    "\n",
    "**Author:** Claire Meyer, Nicolas Furrer, Philipp Schuler\n",
    "**Due date:** 08.05.2020\n",
    "\n",
    "[iapr2018]: https://github.com/LTS5/iapr-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "# tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "# with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "#    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from sympy.solvers import solve\n",
    "from sympy import symbols\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method\n",
    "Using the Bayes method, give the analytical expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value.\n",
    "\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix sigma\n",
    "# Parameter : Data (200x2)(number mesure x dimension), mean of the data (1x2)(1 x dimension)\n",
    "# Return : Covariance matrix sigma (2x2)(dimension x dimension)\n",
    "def covariance(data, mean_value):\n",
    "    X = data-mean_value\n",
    "    M = data.shape[0]\n",
    "    sigma = 1/M*(X.T @ X)\n",
    "    return sigma\n",
    "\n",
    "def pdf(data, data_mu, data_sigma):\n",
    "    p = np.empty((data.shape[0],1))\n",
    "    for i in range(data.shape[0]):\n",
    "        X = data[:][i]-data_mu\n",
    "        sigma_inv = np.linalg.inv(data_sigma)\n",
    "        sigma_det = np.linalg.det(data_sigma)\n",
    "        exp_part = -1/2*(X @ sigma_inv @ X.T)\n",
    "        p[i] = 1/(2*np.pi*np.sqrt(sigma_det))*np.exp(exp_part)\n",
    "    return p\n",
    "\n",
    "def discriminantFonction(data_point, data, Pw):\n",
    "    data_mu = np.mean(data, axis = 0)\n",
    "    data_sigma = covariance(data, data_mu)\n",
    "    data_pdf = pdf(data_point, data_mu, data_sigma)\n",
    "    g = np.log(data_pdf*Pw)\n",
    "    return g\n",
    "\n",
    "def bayesClassification(data_point, class_a, class_b, class_c, Pw):\n",
    "    dist = np.zeros(3)\n",
    "    dist[0] = discriminantFonction(data_point, class_a, Pw)\n",
    "    dist[1] = discriminantFonction(data_point, class_b, Pw)\n",
    "    dist[2] = discriminantFonction(data_point, class_c, Pw)\n",
    "    \n",
    "    return chr(np.argmax(dist)+97) # to get a, b or c class\n",
    "\n",
    "point1 = np.array([[-10,-2]])\n",
    "\n",
    "Pw = 1/3\n",
    "print(bayesClassification(point1,a,b,c,Pw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute different parameter : mean, sigma inverse and parameter c for equation page 12 Cours 5\n",
    "# Parameter : Data (200x2)(number mesure x dimension)\n",
    "# Return : mean (1x2)(1 x dimension, inverse matrix sigma (2x2)(dimension x dimension), parameter c (1x1)\n",
    "def computation(data):\n",
    "    mean = np.mean(data, axis = 0)\n",
    "    sigma = covariance(data, mean)\n",
    "    sigma_det = np.linalg.det(sigma)\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    c = np.log(2*np.pi*np.sqrt(sigma_det))\n",
    "    return mean, sigma_inv, c\n",
    "\n",
    "# Find the decision curves between two data set\n",
    "# Parameter : 2 Data (200x2)(number mesure x dimension)\n",
    "# Return : equation of decision curves (2 equation)\n",
    "def decision_curve(data_i, data_j):\n",
    "    \n",
    "    mean_i, sigma_i_inv, c_i = computation(data_i) \n",
    "    mean_j, sigma_j_inv, c_j = computation(data_j) \n",
    "    \n",
    "    x, y = symbols('x, y', integer=True)\n",
    "    X = np.array([[x,y]])\n",
    "    \n",
    "    g_i = -1/2*((X-mean_i)@sigma_i_inv@(X-mean_i).T)+np.log(1/3)-c_i\n",
    "    g_j = -1/2*((X-mean_j)@sigma_j_inv@(X-mean_j).T)+np.log(1/3)-c_j\n",
    "    g = g_i-g_j\n",
    "    \n",
    "    equ = solve(g[0][0],y)\n",
    "    return equ\n",
    "\n",
    "# Evaluate a equation with the a vector of value x0\n",
    "# Parameter : 1 equation with x as a parameter, vector of value to evaluate the equation\n",
    "# Return : vector of value of y\n",
    "def evaluate_y(equation, x0):\n",
    "    x = symbols('x', integer=True)\n",
    "    y = np.empty(x0.shape[0])\n",
    "    for i in range(x0.shape[0]):\n",
    "        y[i] = equation.subs(x, x0[i])\n",
    "    return y\n",
    "        \n",
    "\n",
    "# Computation of the decision_curve\n",
    "equ1 = decision_curve(a,b)\n",
    "equ2 = decision_curve(b,c)\n",
    "equ3 = decision_curve(c,a)\n",
    "\n",
    "# Computation of y for graph\n",
    "x1 = np.arange(start=-14, stop=14,step=0.01)\n",
    "x2 = np.arange(start=-7.03, stop = 14, step=0.01)\n",
    "x3 = np.arange(start=-7.64, stop = 14, step=0.01)\n",
    "y11 = evaluate_y(equ1[0],x1)\n",
    "y12 = evaluate_y(equ1[1],x1)\n",
    "y21 = evaluate_y(equ2[0],x2)\n",
    "y22 = evaluate_y(equ2[1],x2)\n",
    "y31 = evaluate_y(equ3[0],x3)\n",
    "y32 = evaluate_y(equ3[1],x3)\n",
    "\n",
    "# Plot\n",
    "for i in range(a.shape[0]):\n",
    "    plt.plot(a[i][0],a[i][1],'ro')\n",
    "    plt.plot(b[i][0],b[i][1],'bo')\n",
    "    plt.plot(c[i][0],c[i][1],'go')\n",
    "plt.plot(x1,y11,'c')\n",
    "plt.plot(x1,y12,'c')\n",
    "plt.plot(x2,y21,'m')\n",
    "plt.plot(x2,y22,'m')\n",
    "plt.plot(x3,y31,'k')\n",
    "plt.plot(x3,y32,'k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane.\n",
    "\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mahalanobis distance is defined by :\n",
    "\n",
    "$d_m = \\left( \\left( x-\\mu_i \\right) \\Sigma^{-1}_{i} \\left( x-\\mu_i \\right) \\right)$\n",
    "\n",
    "Where :\n",
    "- $x$ is the point to be classified\n",
    "- $\\mu_i$ is the mean of the $i$-th distribution\n",
    "- $\\Sigma_i$ is the covariance matrix of the $i$-th distribution\n",
    "\n",
    "The Mahalanobis distance is a measure of the distance between a point P and a distribution D, introduced by P. C. Mahalanobis in 1936. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along each principal component axis. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobisDistance(point, data):\n",
    "    mean_value = np.mean(data, axis = 0)\n",
    "    sigma_value = covariance(data, mean_value)\n",
    "    return np.sqrt((point-mean_value)@np.linalg.inv(sigma_value)@(point-mean_value).T)\n",
    "\n",
    "def mahalanobisClassification(point, class_a, class_b, class_c):\n",
    "    dist = np.zeros(3)\n",
    "    dist[0] = mahalanobisDistance(point, class_a)\n",
    "    dist[1] = mahalanobisDistance(point, class_b)\n",
    "    dist[2] = mahalanobisDistance(point, class_c)\n",
    "    \n",
    "    return chr(np.argmin(dist)+97) # to get a, b or c class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "point = np.zeros((1,2))\n",
    "point[0][0] = -10\n",
    "point[0][1] = 0\n",
    "\n",
    "print(mahalanobisClassification(point,a,b,c))\n",
    "\n",
    "# Plot images\n",
    "n_lin = 1\n",
    "n_col = 1\n",
    "size = 6\n",
    "fig, ax = plt.subplots(n_lin, n_col, figsize=(n_col*size, n_lin*size))\n",
    "ax.plot(a[:,0],a[:,1],\".\")\n",
    "ax.plot(b[:,0],b[:,1],\".\")\n",
    "ax.plot(c[:,0],c[:,1],\".\")\n",
    "ax.set_title(\"Classes A, B and C 2D representation\")\n",
    "ax.legend((\"Class a\",\"Class b\",\"Class c\"))\n",
    "\n",
    "# End of cell\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python.\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP\n",
    "*Add your implementation and discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "SciKit proposes a multi-layer perceptron classifier function 'MLPClassifier'. \n",
    "This classifier proposes different architectures and activation functions. \n",
    "We tested all of them to then decide the best classifier for our use case:\n",
    "\n",
    " \n",
    " | Solver | Loss   | Training score | Test score | Beta 2 |\n",
    " |--------|--------|----------------|------------|--------|\n",
    " | Adam   | 0.03   |  0.9915        | 0.9581     | 0.9999 |\n",
    " | Adam   | 0.03   |  0.9848        | 0.9624     | 0.999  |\n",
    " | SGD    | 0.23   |  0.9246        | 0.9171     | N/A    |\n",
    " | LBFGS  | 120.51 |  0.1169        | 0.1075     | N/A    |\n",
    " \n",
    "LBFGD is not optimised for large training sets and this is shown by the tests, as it output random values of loss and score at each iteration. \n",
    "\n",
    "SGD or stochastic gradient descent, is working but takes much more steps to converge than Adam. \n",
    "As adam has the best results without optimisation, and SGD seems to not converging very quickly (200 epochs and not converged) we will use Adam for the nexts steps and try to optimise this one\n",
    " \n",
    "Adam with different activation functions:\n",
    "* Relu: 3.3%\n",
    "* tanh: 5.2%\n",
    "\n",
    "Adam with different alpha values and relu:\n",
    "* 0.0001: 3.3%\n",
    "* 0.001 : 3.9%\n",
    " \n",
    "Adam with different beta_2 values:\n",
    "* 0.999 : 3.3%\n",
    "* 0.9999: 3.4%\n",
    "\n",
    "The changes of parameters are worsening the test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# flattening training and testing set\n",
    "flattened_training_set = train_images.reshape(train_set_size, image_shape[0]*image_shape[1])\n",
    "\n",
    "flattened_testing_set = test_images.reshape(test_set_size,image_shape[0]*image_shape[1])\n",
    "\n",
    "def classifier_compare_plot():\n",
    "    #definition of the used models\n",
    "    model_lbfgs = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100), max_iter=200, random_state=1)\n",
    "\n",
    "    model_adam_1 = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n",
    "                    learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=1, \n",
    "                    tol=0.0001, verbose=False, warm_start=False, early_stopping=True, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.99999, epsilon=1e-08, n_iter_no_change=10)\n",
    "\n",
    "    model_sgd = MLPClassifier(solver='sgd', alpha=1e-4, hidden_layer_sizes=(100), max_iter=100, random_state=1, learning_rate='adaptive')\n",
    "\n",
    "\n",
    "    model_lbfgs.fit(flattened_training_set, train_labels)\n",
    "    model_adam_1.fit(flattened_training_set, train_labels)\n",
    "    model_sgd.fit(flattened_training_set, train_labels)\n",
    "    \n",
    "    return model_lbfgs, model_sgd, model_adam_1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(model):\n",
    "    output = model.predict(flattened_testing_set)\n",
    "    nb_wrong = 0\n",
    "    idx_list = []\n",
    "    for idx, predicted in enumerate(output):\n",
    "        if predicted != test_labels[idx]:\n",
    "            nb_wrong = nb_wrong +1 \n",
    "            idx_list.append(idx)\n",
    "\n",
    "    return 1-(nb_wrong/test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: \tloss =   0.012,\t training score = 0.9954, test score = 0.966\n",
      "SGD: \tloss =   0.182,\t training score = 0.9474, test score = 0.927\n",
      "LBFGS: \tloss = 120.509,\t training score = 0.1169, test score = 0.108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb35676b590>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_lbfgs, model_sgd, model_adam_1 = classifier_compare_plot()\n",
    "\n",
    "test_score_lbfgs = test_score(model_lbfgs)\n",
    "test_score_adam_1 = test_score(model_adam_1)\n",
    "test_score_sgd = test_score(model_sgd)\n",
    "\n",
    "\n",
    "print(\"Adam: \\tloss =   \" + str(round(model_adam_1.loss_, 3)) + \",\\t training score = \" + str(round(model_adam_1.score(flattened_training_set, train_labels),4)) + \",\\t test score = \" + str(round(test_score_adam_1,3)) + \n",
    "          \"\\nSGD: \\tloss =   \" + str(round(model_sgd.loss_,3)) + \",\\t training score = \" + str(round(model_sgd.score(flattened_training_set, train_labels),4)) + \",\\t test score = \" + str(round(test_score_sgd, 3)) + \n",
    "          \"\\nLBFGS: \\tloss = \" + str(round(model_lbfgs.loss_, 3)) + \",\\t training score = \" + str(round(model_lbfgs.score(flattened_training_set, train_labels),4)) + \",\\t test score = \" + str(round(test_score_lbfgs,3)) )\n",
    "\n",
    "plt.plot(model_adam_1.loss_curve_)\n",
    "plt.plot(model_sgd.loss_curve_)\n",
    "plt.legend([\"Adam\", \"SGD\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
